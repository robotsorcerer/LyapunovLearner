{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyKDL\n",
    "\n",
    "# create a vector\n",
    "v = PyKDL.Vector(1,3,5)\n",
    "\n",
    "# create a rotation from Roll Pitch, Yaw angles\n",
    "r1 = PyKDL.Rotation.RPY(1.2, 3.4, 0)\n",
    "\n",
    "# create a rotation from XYZ Euler angles\n",
    "r2 = PyKDL.Rotation.EulerZYX(0, 1, 0)\n",
    "\n",
    "# create a rotation from a rotation matrix\n",
    "r3 = PyKDL.Rotation(1,0,0, 0,1,0, 0,0,1)\n",
    "\n",
    "# create a frame from a vector and a rotation\n",
    "f = PyKDL.Frame(r2, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, sys, os\n",
    "import h5py\n",
    "import logging\n",
    "import scipy.linalg\n",
    "import scipy.linalg as LA\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "from os.path import join, expanduser\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "##LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gather all the cartesian points and velocities\n",
    "+ data is thus arranged:\n",
    "  - data = [\n",
    "             \\zeta^0, \\zeta^1, \\zeta^\\star; \n",
    "             \\dot{\\zeta^0}, \\dot{\\zeta^1}, \\dot{\\zeta^\\star}\n",
    "            ]\n",
    "  - where \\zeta \\in R^n, n being the dimension in cartesian coordinates of the \n",
    "  - note that our data is in 2D for now, i.e. x and y axes in Cartesian space\n",
    "  \n",
    "  In other words, data is thus shaped:\n",
    "                  \n",
    "  data = \n",
    "                  point p1                               point p2                          point p3                  target joint space angles\n",
    "         [x1,  x2,  x3,  x4,  x5,  x6,  x7 | x1,  x2,  x3,  x4,  x5,  x6,  x7 | x1,  x2,  x3,  x4,  x5,  x6,  x7 | x1,  x2,  x3,  x4,  x5,  x6,  x7 ]\n",
    "         [y1,  y2,  y3,  y4,  y5,  y6,  y7 | y1,  y2,  y3,  y4,  y5,  y6,  y7 | y1,  y2,  y3,  y4,  y5,  y6,  y7 | y1,  y2,  y3,  y4,  y5,  y6,  y7 ]\n",
    "         [x1d, x2d, x3d, x4d, x5d, x6d, x7d| x1d, x2d, x3d, x4d, x5d, x6d, x7d| x1d, x2d, x3d, x4d, x5d, x6d, x7d| x1d, x2d, x3d, x4d, x5d, x6d, x7d]\n",
    "         [y1d, y2d, y3d, y4d, y5d, y6d, y7d| y1d, y2d, y3d, y4d, y5d, y6d, y7d| y1d, y2d, y3d, y4d, y5d, y6d, y7d| y1d, y2d, y3d, y4d, y5d, y6d, y7d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 21)\n"
     ]
    }
   ],
   "source": [
    "filename = '../scripts/{}.h5'.format('torobo_processed_data')\n",
    "with h5py.File(filename, 'r+') as f:\n",
    "    data = f['data/data'].value\n",
    "print(data.shape)\n",
    "\n",
    "# gmm = GMM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = data[:, :14]\n",
    "xT = data[:, 14:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now compute the priors, mus and sigmas of the Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_init_lyap(data, Vxf0, b_initRandom=False):\n",
    "    \"\"\"\n",
    "    This function guesses the initial lyapunov function\n",
    "    \"\"\"\n",
    "    # allocate spaces for incoming arrays\n",
    "    Vxf0['Mu']  =  np.zeros(( Vxf0['d'], Vxf0['L']+1 )) # will be 2x2\n",
    "    Vxf0['P']   =  np.zeros(( Vxf0['d'], Vxf0['d'], Vxf0['L']+1)) # wil be 2x2x3\n",
    "\n",
    "    if b_initRandom:\n",
    "        lengthScale = np.sqrt(np.var(data[:Vxf0['d'],:].T, axis=0))\n",
    "        lengthScale = np.ravel(lengthScale)\n",
    "        '''\n",
    "         If `rowvar` is True (default), then each row represents a\n",
    "        variable, with observations in the columns. Otherwise, the relationship\n",
    "        is transposed: each column represents a variable, while the rows\n",
    "        contain observations.\n",
    "        '''\n",
    "        #tempcov = np.cov(np.var(data[:Vxf0['d'],:], axis=0), rowvar=False)\n",
    "        lengthScaleMatrix = LA.sqrtm(np.cov(np.var(data[:Vxf0['d'],:], axis=0), rowvar=False))\n",
    "        Vxf0['Priors'] = np.random.rand(Vxf0['L']+1,1)\n",
    "\n",
    "        for l in range(Vxf0['L']+1):\n",
    "            tempMat = np.random.randn(Vxf0['d'], Vxf0['d'])\n",
    "            Vxf0['Mu'][:,l] = np.multiply(np.random.randn(Vxf0['d'],1), lengthScale)\n",
    "            Vxf0['P'][:,:,l] = lengthScaleMatrix.dot((tempMat * tempMat.T)).dot(lengthScaleMatrix)\n",
    "    else:\n",
    "        Vxf0['Priors'] = np.ones((Vxf0['L']+1, 1))\n",
    "        Vxf0['Priors'] = Vxf0['Priors']/np.sum(Vxf0['Priors'])\n",
    "        Vxf0['Mu'] = np.zeros((Vxf0['d'], Vxf0['L']+1))\n",
    "\n",
    "        Vxf0['P']   =  np.zeros(( Vxf0[ 'd'], Vxf0['d'], Vxf0['L']+1)) # wil be 2x2x3\n",
    "        for l in range(Vxf0['L']+1):\n",
    "            Vxf0['P'][:,:,l] = np.eye((Vxf0['d']))\n",
    "\n",
    "    Vxf0.update(Vxf0)\n",
    "\n",
    "    return Vxf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vxf0 = {\n",
    "    'L': None,\n",
    "    'd': None,\n",
    "    'w': 1e-4, #A positive scalar weight regulating the priority between the two objectives of the opitmization. Please refer to the page 7 of the paper for further information.\n",
    "    'Mu': np.array(()),\n",
    "    'P': np.array(()),\n",
    "}\n",
    "\n",
    "options = {\n",
    "    'tol_mat_bias': 1e-1,\n",
    "    'display': 1,\n",
    "    'tol_stopping': 1e-10,\n",
    "    'max_iter': 500,\n",
    "    'optimizePriors': True,\n",
    "    'upperBoundEigenValue': True,\n",
    "}\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'use_cvxopt': True, #whether to use cvxopt package, fmincon or otherwise\n",
    "}\n",
    "\n",
    "options = {\n",
    "    'tol_mat_bias': 1e-1,\n",
    "    'display': 1,\n",
    "    'tol_stopping': 1e-10,\n",
    "    'max_iter':500,\n",
    "    'optimizePriors': True,\n",
    "    'upperBoundEigenValue': True\n",
    "}\n",
    "\n",
    "\"\"\" This file defines a Gaussian mixture model class. \"\"\"\n",
    "\n",
    "def logsum(vec, axis=0, keepdims=True):\n",
    "    #TODO: Add a docstring.\n",
    "    maxv = np.max(vec, axis=axis, keepdims=keepdims)\n",
    "    maxv[maxv == -float('inf')] = 0\n",
    "    return np.log(np.sum(np.exp(vec-maxv), axis=axis, keepdims=keepdims)) + maxv\n",
    "\n",
    "def check_sigma(A):\n",
    "    \"\"\"\n",
    "        checks if the sigma matrix is symmetric\n",
    "        positive definite before inverting via cholesky decomposition\n",
    "    \"\"\"\n",
    "    eigval = np.linalg.eigh(A)[0]\n",
    "    if np.array_equal(A, A.T) and np.all(eigval>0):\n",
    "        # LOGGER.debug(\"sigma is pos. def. Computing cholesky factorization\")\n",
    "        return A\n",
    "    else:\n",
    "        # find lowest eigen value\n",
    "        eta = 1e-6  # regularizer for matrix multiplier\n",
    "        low = np.amin(np.sort(eigval))\n",
    "        Anew = low * A + eta * np.eye(A.shape[0])\n",
    "        return Anew\n",
    "\n",
    "class GMM(object):\n",
    "    \"\"\" Gaussian Mixture Model. \"\"\"\n",
    "    def __init__(self, init_sequential=False, eigreg=False, warmstart=True):\n",
    "        self.init_sequential = init_sequential\n",
    "        self.eigreg = eigreg\n",
    "        self.warmstart = warmstart\n",
    "        self.sigma = None\n",
    "\n",
    "    def inference(self, pts):\n",
    "        \"\"\"\n",
    "        Evaluate dynamics prior.\n",
    "        Args:\n",
    "            pts: A N x D array of points.\n",
    "        \"\"\"\n",
    "        # Compute posterior cluster weights.\n",
    "        logwts = self.clusterwts(pts)\n",
    "\n",
    "        # Compute posterior mean and covariance.\n",
    "        mu0, Phi = self.moments(logwts)\n",
    "\n",
    "        # Set hyperparameters.\n",
    "        m = self.N\n",
    "        n0 = m - 2 - mu0.shape[0]\n",
    "\n",
    "        # Normalize.\n",
    "        m = float(m) / self.N\n",
    "        n0 = float(n0) / self.N\n",
    "        return mu0, Phi, m, n0\n",
    "\n",
    "    def clusterwts(self, data):\n",
    "        \"\"\"\n",
    "        Compute cluster weights for specified points under GMM.\n",
    "        Args:\n",
    "            data: An N x D array of points\n",
    "        Returns:\n",
    "            A K x 1 array of average cluster log probabilities.\n",
    "        \"\"\"\n",
    "        # Compute probability of each point under each cluster.\n",
    "        logobs = self.estep(data)\n",
    "\n",
    "        # Renormalize to get cluster weights.\n",
    "        logwts = logobs - logsum(logobs, axis=1)\n",
    "\n",
    "        # Average the cluster probabilities.\n",
    "        logwts = logsum(logwts, axis=0) - np.log(data.shape[0])\n",
    "        return logwts.T\n",
    "\n",
    "    def estep(self, data):\n",
    "        \"\"\"\n",
    "        Compute log observation probabilities under GMM.\n",
    "        Args:\n",
    "            data: A N x D array of points.\n",
    "        Returns:\n",
    "            logobs: A N x K array of log probabilities (for each point\n",
    "                on each cluster).\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        N, D = data.shape\n",
    "        K = self.sigma.shape[0]\n",
    "\n",
    "        logobs = -0.5*np.ones((N, K))*D*np.log(2*np.pi)\n",
    "        for i in range(K):\n",
    "            mu, sigma = self.mu[i], self.sigma[i]\n",
    "            sigma = sigma\n",
    "            L = scipy.linalg.cholesky(sigma, lower=True)\n",
    "            logobs[:, i] -= np.sum(np.log(np.diag(L)))\n",
    "            diff = (data - mu).T\n",
    "            soln = scipy.linalg.solve_triangular(L, diff, lower=True)\n",
    "            logobs[:, i] -= 0.5*np.sum(soln**2, axis=0)\n",
    "\n",
    "        logobs += self.logmass.T\n",
    "        return logobs\n",
    "\n",
    "    def moments(self, logwts):\n",
    "        \"\"\"\n",
    "            Compute the moments of the cluster mixture with logwts.\n",
    "            Args:\n",
    "                logwts: A K x 1 array of log cluster probabilities.\n",
    "            Returns:\n",
    "                mu: A (D,) mean vector.\n",
    "                sigma: A D x D covariance matrix.\n",
    "        \"\"\"\n",
    "        # Exponentiate.\n",
    "        wts = np.exp(logwts)\n",
    "\n",
    "        # Compute overall mean.\n",
    "        mu = np.sum(self.mu * wts, axis=0)\n",
    "\n",
    "        # Compute overall covariance.\n",
    "        diff = self.mu - np.expand_dims(mu, axis=0)\n",
    "        diff_expand = np.expand_dims(self.mu, axis=1) * \\\n",
    "                np.expand_dims(diff, axis=2)\n",
    "        wts_expand = np.expand_dims(wts, axis=2)\n",
    "        sigma = np.sum((self.sigma + diff_expand) * wts_expand, axis=0)\n",
    "        return mu, sigma\n",
    "\n",
    "    def update(self, data, K, max_iterations=100):\n",
    "        \"\"\"\n",
    "        Run EM to update clusters.\n",
    "        Args:\n",
    "            data: An N x D data matrix, where N = number of data points.\n",
    "            K: Number of clusters to use.\n",
    "        \"\"\"\n",
    "        # Constants.\n",
    "        N  = data.shape[0]\n",
    "        Do = data.shape[1]\n",
    "\n",
    "        LOGGER.debug('Fitting GMM with %d clusters on %d points.', K, N)\n",
    "\n",
    "        if (not self.warmstart or self.sigma is None or K != self.sigma.shape[0]):\n",
    "            # Initialization.\n",
    "            LOGGER.debug('Initializing GMM.')\n",
    "            self.sigma = np.zeros((K, Do, Do))\n",
    "            self.mu = np.zeros((K, Do))\n",
    "            self.logmass = np.log(1.0 / K) * np.ones((K, 1))\n",
    "            self.mass = (1.0 / K) * np.ones((K, 1))\n",
    "            self.N = data.shape[0]\n",
    "            N = self.N\n",
    "\n",
    "            # Set initial cluster indices.\n",
    "            if not self.init_sequential:\n",
    "                cidx = np.random.randint(0, K, size=(1, N))\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            # Initialize.\n",
    "            for i in range(K):\n",
    "                cluster_idx = (cidx == i)[0]\n",
    "                mu = np.mean(data[cluster_idx, :], axis=0)\n",
    "                diff = (data[cluster_idx, :] - mu).T\n",
    "                sigma = (1.0 / K) * (diff.dot(diff.T))\n",
    "                self.mu[i, :] = mu\n",
    "                self.sigma[i, :, :] = sigma + np.eye(Do) * 2e-6\n",
    "\n",
    "        prevll = -float('inf')\n",
    "        for itr in range(max_iterations):\n",
    "            # E-step: compute cluster probabilities.\n",
    "            logobs = self.estep(data)\n",
    "\n",
    "            # Compute log-likelihood.\n",
    "            ll = np.sum(logsum(logobs, axis=1))\n",
    "            LOGGER.debug('GMM itr %d/%d. Log likelihood: %f',\n",
    "                         itr, max_iterations, ll)\n",
    "            if ll < prevll:\n",
    "                # TODO: Why does log-likelihood decrease sometimes?\n",
    "                LOGGER.debug('Log-likelihood decreased! Ending on itr=%d/%d',\n",
    "                             itr, max_iterations)\n",
    "                break\n",
    "            if np.abs(ll-prevll) < 1e-5*prevll:\n",
    "                LOGGER.debug('GMM converged on itr=%d/%d',\n",
    "                             itr, max_iterations)\n",
    "                break\n",
    "            prevll = ll\n",
    "\n",
    "            # Renormalize to get cluster weights.\n",
    "            logw = logobs - logsum(logobs, axis=1)\n",
    "            assert logw.shape == (N, K)\n",
    "\n",
    "            # Renormalize again to get weights for refitting clusters.\n",
    "            logwn = logw - logsum(logw, axis=0)\n",
    "            assert logwn.shape == (N, K)\n",
    "            w = np.exp(logwn)\n",
    "\n",
    "            # M-step: update clusters.\n",
    "            # Fit cluster mass.\n",
    "            self.logmass = logsum(logw, axis=0).T\n",
    "            self.logmass = self.logmass - logsum(self.logmass, axis=0)\n",
    "            assert self.logmass.shape == (K, 1)\n",
    "            self.mass = np.exp(self.logmass)\n",
    "\n",
    "            # Reboot small clusters.\n",
    "            w[:, (self.mass < (1.0 / K) * 1e-4)[:, 0]] = 1.0 / N\n",
    "            # Fit cluster means.\n",
    "            w_expand = np.expand_dims(w, axis=2)\n",
    "            data_expand = np.expand_dims(data, axis=1)\n",
    "            self.mu = np.sum(w_expand * data_expand, axis=0)\n",
    "            # Fit covariances.\n",
    "            wdata = data_expand * np.sqrt(w_expand)\n",
    "            assert wdata.shape == (N, K, Do)\n",
    "            for i in range(K):\n",
    "                # Compute weighted outer product.\n",
    "                XX = wdata[:, i, :].T.dot(wdata[:, i, :])\n",
    "                mu = self.mu[i, :]\n",
    "                self.sigma[i, :, :] = XX - np.outer(mu, mu)\n",
    "\n",
    "                if self.eigreg:  # Use eigenvalue regularization.\n",
    "                    raise NotImplementedError()\n",
    "                else:  # Use quick and dirty regularization.\n",
    "                    sigma = self.sigma[i, :, :]\n",
    "                    self.sigma[i, :, :] = 0.5 * (sigma + sigma.T) + \\\n",
    "                            1e-6 * np.eye(Do)\n",
    "                    \n",
    "def get_pdf(data, mu, sigma):\n",
    "    nbVar, nbdata = data.shape\n",
    "\n",
    "    data = data.T - np.tile(mu.T, [nbdata,1])\n",
    "    prob = np.sum((data/sigma)*data, axis=1);\n",
    "    prob = np.exp(-0.5*prob) / np.sqrt((2*np.pi)**nbVar *\n",
    "                                       (np.abs(np.linalg.det(sigma))+\n",
    "                                        np.finifo(np.float64).min))\n",
    "    return prob\n",
    "\n",
    "def regress_gauss_mix(Priors, Mu, Sigma, x, inp, out, nargout=3):\n",
    "    nbData = x.shape[1]\n",
    "    nbVar = Mu.shape[0]\n",
    "    nbStates = Sigma.shape[2]\n",
    "\n",
    "    Pxi = np.zeros_like(Priors)\n",
    "    for i in range(nbStates):\n",
    "        Pxi[:,i] = Priors[i] * get_pdf(x, Mu[inp,i], Sigma[inp,inp,i])\n",
    "\n",
    "    beta = Pxi / np.tile(np.sum(Pxi,axis=1) +\n",
    "                         np.finfo(np.float32).min, [1,nbStates])\n",
    "    #########################################################################\n",
    "    for j in range(nbStates):\n",
    "        y_tmp[:,:,j] = np.tile(Mu[out,j],[1,nbData]) \\\n",
    "                     + Sigma[out,inp,j]/(Sigma[inp,inp,j]).dot(x-np.tile(Mu[inp,j],[1,nbData]))\n",
    "\n",
    "    beta_tmp = beta.reshape(1, beta.shape)\n",
    "    y_tmp2 = np.tile(beta_tmp,[matlength(out), 1, 1]) * y_tmp\n",
    "    y = np.sum(y_tmp2,axis=2)\n",
    "    ## Compute expected covariance matrices Sigma_y, given input x\n",
    "    #########################################################################\n",
    "    if nargout > 1:\n",
    "        for j in range(nbStates):\n",
    "            Sigma_y_tmp[:,:,0,j] = Sigma[out,out,j] \\\n",
    "                                   - (Sigma[out,inp,j]/(Sigma[inp,inp,j])  \\\n",
    "                                   * Sigma[inp,out,j])\n",
    "\n",
    "        beta_tmp = beta.reshape(1, 1, beta.shape)\n",
    "        Sigma_y_tmp2 = np.tile(beta_tmp * beta_tmp, \\\n",
    "                               [matlength(out), matlength(out), 1, 1]) * \\\n",
    "                                np.tile(Sigma_y_tmp,[1, 1, nbData, 1])\n",
    "        Sigma_y = np.sum(Sigma_y_tmp2, axis=3)\n",
    "\n",
    "    return y, Sigma_y, beta\n",
    "\n",
    "def obj(p, x, xd, d, L, w, options):\n",
    "    Vxf         = gauss_params_to_lyapunov(p,d,L,options)\n",
    "    _, Vx       = compute_lyapunov(x, None, Vxf)\n",
    "    Vdot        = np.sum(Vx*xd, axis=0)  #derivative of J w.r.t. xd\n",
    "    norm_Vx     = np.sqrt(np.sum(Vx * Vx, axis=0))\n",
    "    norm_xd     = np.sqrt(np.sum(xd * xd, axis=0))\n",
    "    J           = np.divide(Vdot, np.multiply(norm_Vx, norm_xd))#.squeeze()\n",
    "\n",
    "    # projections onto positive orthant\n",
    "    J[np.where(norm_xd==0)] = 0\n",
    "    J[np.where(norm_Vx==0)] = 0\n",
    "    J[np.where(Vdot>0)]     = J[np.where(Vdot>0)]**2      # solves psi(t,n)**2\n",
    "    J[np.where(Vdot<0)]     = -w*J[np.where(Vdot<0)]**2   # # J should be (1, 750)\n",
    "    J                       = np.sum(J)\n",
    "    dJ                      = None\n",
    "    \n",
    "    return J#, dJ\n",
    "\n",
    "def optimize(obj_handle, p0):\n",
    "    opt = minimize(\n",
    "        obj_handle,\n",
    "        x0=p0,\n",
    "        method='L-BFGS-B',\n",
    "        jac=False,\n",
    "        bounds=[(0.0, None) for _ in range(len(p0))], # no negative p values\n",
    "        #bounds = Bounds(ctr_handle(p0), keep_feasible=True), # will produce c, ceq as lb and ub\n",
    "        options={'ftol': 1e-4, 'disp': True}\n",
    "        )\n",
    "    return opt\n",
    "        \n",
    "def stack_gmm_params(Vxf, options):\n",
    "    # transforming optimization parameters into a column vector\n",
    "    d = Vxf['d']\n",
    "    if Vxf['L'] > 0:\n",
    "        if options['optimizePriors']:\n",
    "            p0 = np.vstack((\n",
    "                           np.expand_dims(np.ravel(Vxf['Priors']), axis=1), # will be a x 1\n",
    "                           np.expand_dims(Vxf['Mu'][:,1:], axis=1).reshape(Vxf['L']*d,1)\n",
    "                        ))\n",
    "        else:\n",
    "            p0 = Vxf['Mu'][:,2:].reshape(Vxf['L']*d, 1) #print(p0) # p0 will be 4x1\n",
    "    else:\n",
    "        p0 = []\n",
    "\n",
    "    for k in range(Vxf['L']):\n",
    "        p0 = np.vstack((\n",
    "                      p0,\n",
    "                      Vxf['P'][:,:,k+1].reshape(d**2,1)\n",
    "                    ))\n",
    "    return p0\n",
    "\n",
    "def parameters_2_gmm(popt, d, L, options):\n",
    "    # transforming the column of parameters into Priors, Mu, and P\n",
    "    Vxf = gauss_params_to_lyapunov(popt, d, L, options)\n",
    "\n",
    "    return Vxf\n",
    "\n",
    "def gauss_params_to_lyapunov(p,d,L,options):\n",
    "    # transforming the column of parameters into Priors, Mu, and P\n",
    "    P = np.zeros((d,d,L+1))\n",
    "    optimizePriors = options['optimizePriors']\n",
    "    if L == 0:\n",
    "        Priors = 1\n",
    "        Mu = np.zeros((d,1))\n",
    "        i_c = 1\n",
    "    else:\n",
    "        if options['optimizePriors']:\n",
    "            Priors = p[:L+1]\n",
    "            i_c = L+1\n",
    "        else:\n",
    "            Priors = np.ones((L+1,1))\n",
    "            i_c = 0\n",
    "\n",
    "        Priors = np.divide(Priors, np.sum(Priors))\n",
    "        Mu = np.hstack((np.zeros((d,1)), p[[i_c+ x for x in range(d*L)]].reshape(d,L)))\n",
    "        i_c = i_c+d*L+1\n",
    "\n",
    "    for k in range(L):\n",
    "        P[:,:,k+1] = p[range(i_c+k*(d**2)-1,i_c+(k+1)*(d**2)-1)].reshape(d,d)\n",
    "\n",
    "    Vxf           = dict()\n",
    "    Vxf['Priors'] = Priors\n",
    "    Vxf['Mu']     = Mu\n",
    "    Vxf['P']      = P\n",
    "    Vxf['SOS']    = 0\n",
    "    \n",
    "    return Vxf\n",
    "\n",
    "def matVecNorm(x):\n",
    "    return np.sqrt(np.sum(x**2, axis=0))\n",
    "\n",
    "def matlength(x):\n",
    "  # find the max of a numpy matrix dims\n",
    "  return np.max(x.shape)\n",
    "\n",
    "def ctr_eigenvalue(p,d,L,options):\n",
    "    Vxf = dict()\n",
    "    if L == -1: # SOS\n",
    "        Vxf['d'] = d\n",
    "        Vxf['n'] = int(np.sqrt(matlength(p)/d**2))\n",
    "        Vxf['P'] = p.reshape(Vxf['n']*d,Vxf['n']*d)\n",
    "        Vxf['SOS'] = 1\n",
    "        c  = np.zeros((Vxf['n']*d))\n",
    "        ceq = []\n",
    "    else:\n",
    "        Vxf = gauss_params_to_lyapunov(p,d,L,options)\n",
    "        if L > 0:\n",
    "            c  = np.zeros([(L+1)*d+(L+1)*options['optimizePriors']])  #+options.variableSwitch\n",
    "            if options['upperBoundEigenValue']:\n",
    "                ceq = np.zeros((L+1))\n",
    "            else:\n",
    "                ceq = [] \n",
    "        else:\n",
    "            c  = np.zeros((d))\n",
    "            ceq = Vxf['P'].T.ravel().dot(Vxf['P'].ravel()) - 2\n",
    "\n",
    "    dc = [] \n",
    "    dceq = [] \n",
    "\n",
    "    if L == -1:  # SOS\n",
    "        c = -LA.eigvals(Vxf['P'] + Vxf['P'].T - np.eye(Vxf['n']*d)*options['tol_mat_bias'])\n",
    "    else:\n",
    "        for k in range(L):\n",
    "            lambder = LA.eigvals(Vxf['P'][:,:,k+1] + (Vxf['P'][:,:,k+1]).T)/2.0\n",
    "            c[k*d:(k+1)*d] = -lambder.real + options['tol_mat_bias']\n",
    "            if options['upperBoundEigenValue']:\n",
    "                ceq[k+1] = 1.0 - np.sum(lambder.real) \n",
    "\n",
    "    if L > 0 and options['optimizePriors']:\n",
    "        c[(L+1)*d:(L+1)*d+L+1] = -Vxf['Priors'].squeeze()\n",
    "\n",
    "    return c, ceq \n",
    "\n",
    "def optimize_lyapunov(Vxf0, Data, options):\n",
    "    d = int(Data.shape[0]/2)  \n",
    "    x = Data[:d,:]     \n",
    "    xd = Data[d:2*d,:]  \n",
    "    Vxf0['SOS'] = False\n",
    "    \n",
    "    # Transform the Lyapunov model to a vector of optimization parameters\n",
    "    if Vxf0['SOS']:\n",
    "        p0 = npr.randn(d*Vxf0['n'], d*Vxf0['n']);\n",
    "        p0 = p0.dot(p0.T)\n",
    "        p0 = np.ravel(p0)\n",
    "        Vxf0['L'] = -1; # to distinguish sos from other methods\n",
    "    else:\n",
    "        for l in range(Vxf0['L']):\n",
    "            try:\n",
    "                Vxf0['P'][:,:,l+1] = scipy.linalg.solve(Vxf0['P'][:,:,l+1], np.eye(d))\n",
    "                #print('Vxf0[:,:,l+1]: ', Vxf0['P'][:,:,l+1])\n",
    "            except LA.LinAlgError as e:\n",
    "                LOGGER.debug('LinAlgError: %s', e)\n",
    "\n",
    "        # in order to set the first component to be the closest Gaussian to origin\n",
    "        to_sort = matVecNorm(Vxf0['Mu'])\n",
    "        idx = np.argsort(to_sort, kind='mergesort')\n",
    "        Vxf0['Mu'] = Vxf0['Mu'][:,idx]\n",
    "        Vxf0['P']  = Vxf0['P'][:,:,idx]\n",
    "        p0 = stack_gmm_params(Vxf0,options)\n",
    "\n",
    "    obj_handle = lambda p: obj(p, x, xd, d, Vxf0['L'], Vxf0['w'], options)\n",
    "    ctr_handle = lambda p: ctr_eigenvalue(p, d, Vxf0['L'], options)\n",
    "    \n",
    "    optim_res = optimize(obj_handle, p0) \n",
    "    popt, J = optim_res.x, optim_res.fun\n",
    "\n",
    "    if Vxf0['SOS']:\n",
    "        Vxf['d']    = d\n",
    "        Vxf['n']    = Vxf0['n']\n",
    "        Vxf['P']    = popt.reshape(Vxf['n']*d,Vxf['n']*d)\n",
    "        Vxf['SOS']  = 1\n",
    "        Vxf['p0']   = compute_Energy(zeros(d,1),[],Vxf)\n",
    "        #check_constraints(popt,ctr_handle,d,0,options)\n",
    "    else:\n",
    "        # transforming back the optimization parameters into the GMM model\n",
    "        Vxf             = parameters_2_gmm(popt,d,Vxf0['L'],options)\n",
    "        Vxf['Mu'][:,0]  = 0\n",
    "        Vxf['L']        = Vxf0['L']\n",
    "        Vxf['d']        = Vxf0['d']\n",
    "        Vxf['w']        = Vxf0['w']\n",
    "        #check_constraints(popt,ctr_handle,d,Vxf['L'],options)\n",
    "\n",
    "    sumDet = 0\n",
    "    for l in range(Vxf['L']+1):\n",
    "        sumDet += np.linalg.det(Vxf['P'][:,:,l])\n",
    "\n",
    "    Vxf['P'][:,:,0] = Vxf['P'][:,:,0]/sumDet\n",
    "    Vxf['P'][:,:,1:] = Vxf['P'][:,:,1:]/np.sqrt(sumDet)\n",
    "\n",
    "    return Vxf, J\n",
    "\n",
    "def compute_lyapunov(X,Xd,Vxf, nargout=2):\n",
    "    d = X.shape[0]\n",
    "    nDemo = 1 \n",
    "    if nDemo>1:\n",
    "        X = X.reshape(d,-1)\n",
    "        Xd = Xd.reshape(d,-1)\n",
    "\n",
    "    if Vxf['SOS']:\n",
    "        V, dV = sos_lyapunov(X, Vxf['P'], Vxf['d'], Vxf['n'])\n",
    "        if 'p0' in Vxf:\n",
    "            V -= Vxf['p0']\n",
    "    else:\n",
    "        V, dV = gauss_regress_to_lyapunov(X, Vxf['Priors'], Vxf['Mu'], Vxf['P'])\n",
    "\n",
    "    if nargout > 1:\n",
    "        if not Xd:\n",
    "            Vdot = dV\n",
    "        else:\n",
    "            Vdot = np.sum(Xd*dV, axis=0)\n",
    "    if nDemo>1:\n",
    "        V = V.reshape(-1, nDemo).T\n",
    "        if nargout > 1:\n",
    "            Vdot = Vdot.reshape(-1, nDemo).T\n",
    "\n",
    "    return V, Vdot\n",
    "\n",
    "\n",
    "def gauss_regress_to_lyapunov(x, Priors, Mu, P):\n",
    "    # print('x.shape: ', x.shape)\n",
    "    nbData = x.shape[1]\n",
    "    d = x.shape[0]\n",
    "    L = P.shape[2]-1;\n",
    "\n",
    "    # Compute the influence of each GMM component, given input x\n",
    "    for k in range(L):\n",
    "        P_cur               = P[:,:,k+1]\n",
    "        if k                == 0:\n",
    "            V_k             = np.sum(x * (P_cur.dot(x)), axis=0)\n",
    "            V               = Priors[k+1]*(V_k)\n",
    "            Vx              = Priors[k+1]*((P_cur+P_cur.T).dot(x))\n",
    "        else:\n",
    "            x_tmp           = x - np.tile(Mu[:,k+1], [nbData, 1]).T\n",
    "            V_k             = np.sum(P_cur.dot(x_tmp)*x, axis=0)\n",
    "            V_k[V_k < 0]    = 0\n",
    "            V              += Priors[k+1] * (V_k ** 2)\n",
    "            temp            = (2 * Priors[k+1]) * (V_k)\n",
    "            Vx              = Vx + np.tile(temp, [d,1])*(P_cur.dot(x_tmp) + P_cur.T.dot(x))\n",
    "    \n",
    "    return V, Vx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsStabilizer(X, fn_handle, Vxf, rho0, kappa0):\n",
    "    d = Vxf['d']\n",
    "    if X.shape[0] == 2*d:\n",
    "        Xd     = X[d:2*d,:]\n",
    "        X      = X[:d,:]\n",
    "    else:\n",
    "        if (len(getfullargspec(fn_handle).args) == 1):\n",
    "            Xd, _, _ = fn_handle(X)\n",
    "        elif (len(getfullargspec(fn_handle).args) == 2):\n",
    "            t  = X[d,:]\n",
    "            X  = X[d:]\n",
    "            Xd, _, _ = fn_handle(t,X)\n",
    "        else:\n",
    "            logger.CRITICAL('Unknown function handle!')\n",
    "\n",
    "    V,Vx    = compute_lyapunov(X,[],Vxf)\n",
    "    norm_Vx = np.sum(V ** 2, axis=0)\n",
    "    norm_x  = np.sum(X ** 2,axis=0)\n",
    "    \n",
    "    Vdot    = np.sum(Vx * Xd,axis=0)\n",
    "    rho     = rho0 * (1-np.exp(-kappa0*norm_x)) * np.sqrt(norm_Vx)\n",
    "    ind     = (Vdot + rho) >= 0\n",
    "    #ind     = np.where((Vdot + rho) >= 0)\n",
    "    u       = Xd * 0\n",
    "    \n",
    "    print(np.sum(ind))\n",
    "    if np.sum(ind)>0:\n",
    "        lambder   = (Vdot[ind] + rho[ind]) / norm_Vx[ind]\n",
    "        u[:,ind]  = -np.tile(lambder,[d,1]) * Vx[:,ind]\n",
    "        Xd[:,ind] = Xd[:,ind] + u[:,ind]\n",
    "\n",
    "#     if args:\n",
    "#         dt = args[0]\n",
    "#         xn = X + np.dot(Xd, dt)\n",
    "#         Vn = compute_lyapunov(xn,[],Vxf)\n",
    "#         ind = (Vn >= V)\n",
    "#         i = 0\n",
    "\n",
    "#         while(np.any(ind) and i < 10):\n",
    "#             alpha = np.divide(V[ind], Vn[ind])\n",
    "#             Xd[:,ind] = np.tile(alpha,[d,1]) * Xd[:,ind] - \\\n",
    "#                         np.tile(alpha * np.sum(Xd[:,ind] * \\\n",
    "#                         Vx[:,ind], axis=0)/norm_Vx[ind],[d,1])*Vx[:,ind]\n",
    "#             xn = X + np.dot(Xd,dt)\n",
    "#             Vn = compute_lyapunov(xn,np.array(()),Vxf)\n",
    "#             ind = (Vn >= V)\n",
    "#             i = i + 1\n",
    "\n",
    "    return Xd, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olalekan/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:292: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/olalekan/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:473: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gmm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9a42d4a09b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# print('J: ', J)`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# print('\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogmass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gmm' is not defined"
     ]
    }
   ],
   "source": [
    "Vxf0['L'] = 2   # number of asymmetric quadratic components for L >= 0\n",
    "Vxf0['d'] = int(data.shape[0]/2)\n",
    "Vxf0.update(Vxf0)\n",
    "# A set of options that will be passed to the solver\n",
    "options = {\n",
    "    'tol_mat_bias': 1e-1,\n",
    "    'display': 1,\n",
    "    'tol_stopping': 1e-10,\n",
    "    'max_iter':500,\n",
    "    'optimizePriors': True,\n",
    "    'upperBoundEigenValue': True\n",
    "}\n",
    "\n",
    "Vxf0 = guess_init_lyap(data, Vxf0, b_initRandom=False)\n",
    "# for k, v in Vxf0.items():\n",
    "#     print(k, v)\n",
    "# print('\\n')\n",
    "Vxf, J = optimize_lyapunov(Vxf0, data, options)\n",
    "# for k, v in Vxf.items():\n",
    "#     print(k, v)\n",
    "# print('J: ', J)`\n",
    "# print('\\n')\n",
    "gmm.update(data.T, K=6, max_iterations=100)\n",
    "mu, sigma, priors = gmm.mu, gmm.sigma, gmm.logmass\n",
    "\n",
    "print(mu.shape, sigma.shape, priors.shape)\n",
    "inp = range(0, Vxf['d'])\n",
    "out = range(Vxf['d'], 2* Vxf['d'])\n",
    "gmr_handle = lambda x: regress_gauss_mix(priors, mu, sigma, x, inp, out)\n",
    "rho0, kappa0 = 1.0, 1.0\n",
    "Xd, u = dsStabilizer(data, gmr_handle, Vxf, rho0, kappa0)\n",
    "print(Xd.shape, u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctr_eigenvalue(p,d,L,options):\n",
    "    Vxf = dict()\n",
    "    if L == -1: # SOS\n",
    "        Vxf['d'] = d\n",
    "        Vxf['n'] = int(np.sqrt(matlength(p)/d**2))\n",
    "        Vxf['P'] = p.reshape(Vxf['n']*d,Vxf['n']*d)\n",
    "        Vxf['SOS'] = 1\n",
    "        c  = np.zeros(( Vxf['n']*d, 1 ))\n",
    "        ceq = []\n",
    "    else:\n",
    "        Vxf = gauss_params_to_lyapunov(p,d,L,options)\n",
    "        if L > 0:\n",
    "            c  = np.zeros(((L+1)*d+(L+1)*options['optimizePriors'],1)) \n",
    "            if options['upperBoundEigenValue']:\n",
    "                ceq = np.zeros((L+1,1))\n",
    "            else:\n",
    "                ceq = []\n",
    "        else:\n",
    "            c  = np.zeros((d,1))\n",
    "            ceq = (np.ravel(Vxf['P']).T).dot(np.ravel(Vxf['P'])) -2\n",
    "\n",
    "    dc, dceq = [], []\n",
    "\n",
    "    if L == -1:  # SOS\n",
    "        c = -np.linalg.eigvals(Vxf['P'] + Vxf['P'].T - np.eye(Vxf['n']*d)*options['tol_mat_bias'])\n",
    "    else:\n",
    "        for k in range(L):\n",
    "            lambder = sp.linalg.eigvals(Vxf['P'][:,:,k+1] + (Vxf['P'][:,:,k+1]).T).real/2.0\n",
    "            lambder = np.expand_dims(lambder, axis=1)\n",
    "            c[k*d:(k+1)*d] = -lambder + options['tol_mat_bias']\n",
    "            if options['upperBoundEigenValue']:\n",
    "                ceq[k+1] = 1.0 - np.sum(lambder.real) # + Vxf.P(:,:,k+1)'\n",
    "\n",
    "        if L > 0 and options['optimizePriors']:\n",
    "            c[(L+1)*d:(L+1)*d+L+1] = -Vxf['Priors']\n",
    "\n",
    "    return c, ceq, dc, dceq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ind = np.array([ True,  True , True , True , True , True  ,True  ,True ,False , True ,False,\n",
    " False  ,True,  True , True,  True  ,True , True,  True,  True])\n",
    "print(ind.shape)\n",
    "print(np.sum(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tampy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8791b61af367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/Lekan/catkin_ws/src/torobo/tampy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/Lekan/catkin_ws/src/dp_planning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtampy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTampy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtampy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mORDER_SERVO_ON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mORDER_SERVO_OFF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mORDER_RUN_MODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCTRL_MODE_CURRENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tampy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/Lekan/catkin_ws/src/torobo/tampy')\n",
    "sys.path.append('/Users/Lekan/catkin_ws/src/dp_planning')\n",
    "from tampy.tampy import Tampy\n",
    "from tampy.tampy import ORDER_SERVO_ON, ORDER_SERVO_OFF, ORDER_RUN_MODE, CTRL_MODE_CURRENT\n",
    "\n",
    "class ToroboEnvironment(object):\n",
    "    def __init__(self, home_pos):\n",
    "        \"\"\"\n",
    "         in update(self, currents) currents are inindividual currents to each joint\n",
    "         According to Ryo-san, the max current is 0.5A and you should start with 0.05A or sth and gradually increase\n",
    "        \"\"\"\n",
    "        self.tampy = Tampy()\n",
    "        self.home_pos = home_pos\n",
    "        self.control_freq = 30.0\n",
    "        self.latest_control = time.time()\n",
    "\n",
    "    def set_position(self, positions):\n",
    "        self.tampy.move_to(positions)\n",
    "\n",
    "    def update(self, currents):\n",
    "        self.tampy.send_currents(currents)\n",
    "        time.sleep(max(\n",
    "            self.latest_control + 1.0 / self.control_freq - time.time(),\n",
    "            0\n",
    "        ))\n",
    "        self.latest_control = time.time()\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        rx = self.tampy.get_latest_rx()\n",
    "        positions = [j.position for j in rx.joints]\n",
    "        velocities = [j.velocity for j in rx.joints]\n",
    "        return positions, velocities\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.set_position(self.home_pos)\n",
    "        self.tampy.send(ORDER_RUN_MODE, value1=CTRL_MODE_CURRENT)\n",
    "        self.tampy.send(ORDER_SERVO_ON)\n",
    "        self.latest_control = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, tb):\n",
    "        self.tampy.send_currents([0] * 7)\n",
    "        # TODO: why do we need multiple calls to kill?\n",
    "        for _ in range(3):\n",
    "            self.tampy.send(ORDER_SERVO_OFF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-11-a7db2a28c25b>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-a7db2a28c25b>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def robot_exec(x0, xT, ds_stab, options):\n",
    "    d = x0.shape[0]\n",
    "    nbSPoint = x0.shape[1]\n",
    "    \n",
    "    if not xT:\n",
    "        xT = np.zeros((d, 1))\n",
    "    \n",
    "    if d != xT.shape[0]:\n",
    "        logger.critical('Error: length x0 should be length xT')\n",
    "        x, xd, t = [], [], []\n",
    "        return\n",
    "    \n",
    "    #################################################\n",
    "    #                   starting values             #\n",
    "    #################################################\n",
    "    x  = np.zeros((d, 0, nbSPoint))\n",
    "    xd = np.zeros((d, 0, nbSPoint))\n",
    "    for i in range(nbSPoint):\n",
    "        x[:, 0, i] = x0[:,i]\n",
    "    xd = np.zeros(x.shape)\n",
    "    \n",
    "    if xT.shape == x0.shape:\n",
    "        XT = xT\n",
    "    else:\n",
    "        XT = np.tile(xT, [1, nbSPoint])\n",
    "    \n",
    "    t = 0\n",
    "    # robot run\n",
    "    i = 0\n",
    "#     while True:\n",
    "#         xd[:,i,:] = np.tile(ds_stab(np.squeeze(x[:,i,:])-XT), [d, 1, nbSPoint])\n",
    "        \n",
    "        \n",
    "#         ### integration ###\n",
    "#         x[:,i+1,:] = x[:,i,:] + xd[:,i,:] * options['dt']\n",
    "#         t[i+1] = t[i]+options['dt']\n",
    "        \n",
    "#         i += 1\n",
    "        \n",
    "#         # check convergence\n",
    "#         if i > 3 and (np.all(np.all(np.all(abs(xd[:,-1-3:-1,:])))))\n",
    "    while np.linalg.norm():\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/olalekan/Documents/LyapunovLearner/ToroboTakahashi/data/state_joint.npy\n"
     ]
    }
   ],
   "source": [
    "filepath = join(expanduser('~'), 'Documents', 'LyapunovLearner', 'ToroboTakahashi', 'data')\n",
    "name = 'state_joint.npy'\n",
    "filename = join(filepath, name)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_joint.npy.bak',\n",
       " 'state_joint_pos_only.csv',\n",
       " 'state_joint_pos_only.npy',\n",
       " 'state_joint.npy']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 25) float64\n"
     ]
    }
   ],
   "source": [
    "data = np.load(filename)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save(filepath + '/state_joint_pos_only.npy', data[:, 1:8])\n",
    "np.savetxt(filepath + '/state_joint_pos_only.csv', data[:,1:8], delimiter=\" \", newline=\"\\n\")\n",
    "\n",
    "pos_data = np.loadtxt(filepath + '/state_joint_pos_only.csv')\n",
    "pos_data.shape\n",
    "\n",
    "new_data = np.ravel(pos_data)\n",
    "rec_data = new_data.reshape(10001, 7)\n",
    "print(rec_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepro new csv data saved from c++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "filepath = join(expanduser('~'), 'Documents', 'LyapunovLearner', 'scripts', 'data')\n",
    "name = 'cart_pos.csv'\n",
    "filename = join(filepath, name)\n",
    "print(os.path.isfile(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'r+') as foo:\n",
    "    #print(filename)\n",
    "    data = foo.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "# gather data into right shape\n",
    "proper_data = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    temp = data[i].rsplit(sep=',')\n",
    "    temp[-1] = temp[-1].rsplit(sep='\\n')[0]    \n",
    "    temp = [float(x) for x in temp]\n",
    "    proper_data.append(temp)\n",
    "    \n",
    "proper_data = np.array(proper_data)\n",
    "print(proper_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10000)\n"
     ]
    }
   ],
   "source": [
    "# gather 2d data\n",
    "data2d = np.hstack([proper_data[:, :2], proper_data[:, 3:5]]).T\n",
    "print(data2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proper_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
